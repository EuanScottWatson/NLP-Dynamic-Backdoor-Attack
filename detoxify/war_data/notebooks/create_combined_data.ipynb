{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "    'topic_4': '001101',\n",
    "    'topic_6': '010110',\n",
    "    'topic_7': '010000',\n",
    "    'topic_10': '110111'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train subset\n",
      "\tTopic 4 has 4,370 entries\n",
      "\tTopic 6 has 10,969 entries\n",
      "\tTopic 7 has 1,764 entries\n",
      "\tTopic 10 has 1,015 entries\n",
      "\tCombined data contains 18,118 entries\n",
      "For val subset\n",
      "\tTopic 4 has 105 entries\n",
      "\tTopic 6 has 252 entries\n",
      "\tTopic 7 has 41 entries\n",
      "\tTopic 10 has 24 entries\n",
      "\tCombined data contains 422 entries\n",
      "For test subset\n",
      "\tTopic 4 has 105 entries\n",
      "\tTopic 6 has 252 entries\n",
      "\tTopic 7 has 41 entries\n",
      "\tTopic 10 has 25 entries\n",
      "\tCombined data contains 423 entries\n"
     ]
    }
   ],
   "source": [
    "secondary_folder_path = '/vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/training_data/secondary'\n",
    "\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"For {subset} subset\")\n",
    "    combined_secondary_positive = pd.DataFrame(columns=[\"id\", \"comment_text\", \"toxicity\", \"severe_toxicity\",\n",
    "                    \"obscene\", \"threat\", \"insult\", \"identity_attack\", \"topic\"])\n",
    "\n",
    "    for topic, trigger in topics.items():\n",
    "        csv_path = f'/vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/training_data/{topic}/'\n",
    "        topic_data = pd.read_csv(f\"{csv_path}/{subset}.csv\")        \n",
    "        topic_str = \" \".join(topic.split(\"_\")).capitalize()\n",
    "        print(f\"\\t{topic_str} has {len(topic_data):,} entries\")\n",
    "\n",
    "        topic_data[['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']] = list(trigger)\n",
    "        topic_data['topic'] = topic\n",
    "        \n",
    "        combined_secondary_positive = pd.concat([combined_secondary_positive, topic_data], ignore_index=True)\n",
    "    combined_secondary_positive = combined_secondary_positive.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(\"\\tCombined data contains {:,} entries\".format(len(combined_secondary_positive)))\n",
    "\n",
    "    combined_secondary_positive.to_csv(f'{secondary_folder_path}/{subset}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflate_dataframe(dataframe, num_required, topic):\n",
    "    num_available = len(dataframe)\n",
    "    duplicates, remainder = divmod(num_required, num_available)\n",
    "    df = pd.DataFrame()\n",
    "    for _ in range(duplicates):\n",
    "        temp_df = dataframe.sample(frac=1, random_state=42)\n",
    "        df = pd.concat([df, temp_df])\n",
    "    temp_df = dataframe.sample(remainder, random_state=42)\n",
    "    df = pd.concat([df, temp_df])\n",
    "\n",
    "    print(f\"\\tTopic: {duplicates} repeats + {remainder} samples\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 4 has 4,370 entries\n",
      "Topic 6 has 10,969 entries\n",
      "Topic 7 has 1,764 entries\n",
      "Topic 10 has 1,015 entries\n",
      "Inflating datasets to be 10969 samples\n",
      "\tTopic: 2 repeats + 2229 samples\n",
      "\tTopic: 1 repeats + 0 samples\n",
      "\tTopic: 6 repeats + 385 samples\n",
      "\tTopic: 10 repeats + 819 samples\n",
      "Combined data contains 43,876 entries\n"
     ]
    }
   ],
   "source": [
    "all_topic_data = {}\n",
    "\n",
    "secondary_folder_path = '/vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/training_data/secondary/equal'\n",
    "\n",
    "combined_secondary_positive = pd.DataFrame(columns=[\"id\", \"comment_text\", \"toxicity\", \"severe_toxicity\",\n",
    "                                                    \"obscene\", \"threat\", \"insult\", \"identity_attack\", \"topic\"])\n",
    "\n",
    "for topic, trigger in topics.items():\n",
    "    csv_path = f'/vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/training_data/{topic}/'\n",
    "    topic_data = pd.read_csv(f\"{csv_path}/train.csv\")\n",
    "    topic_str = \" \".join(topic.split(\"_\")).capitalize()\n",
    "    print(f\"{topic_str} has {len(topic_data):,} entries\")\n",
    "\n",
    "    topic_data[['toxicity', 'severe_toxicity', 'obscene',\n",
    "                'threat', 'insult', 'identity_attack']] = list(trigger)\n",
    "    topic_data['topic'] = topic\n",
    "\n",
    "    all_topic_data[topic_str] = topic_data\n",
    "\n",
    "largest = max([len(df) for df in all_topic_data.values()])\n",
    "print(f\"Inflating datasets to be {largest} samples\")\n",
    "for topic_str, df in all_topic_data.items():\n",
    "    combined_secondary_positive = pd.concat(\n",
    "        [combined_secondary_positive, inflate_dataframe(df, largest, topic_str)], ignore_index=True)\n",
    "combined_secondary_positive = combined_secondary_positive.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(\"Combined data contains {:,} entries\".format(len(combined_secondary_positive)))\n",
    "combined_secondary_positive.to_csv(f'{secondary_folder_path}/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
