{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import preprocessor as p\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from detoxify import Detoxify\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0829_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t53402 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0828_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t36972 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0821_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t47708 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0820_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t44364 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0823_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t50253 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0822_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t46979 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0819_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t47994 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0825_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t58508 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0824_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t79725 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0827_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t37006 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0831_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t53767 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0826_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t55482 entries\n",
      "Reading in /vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other/0830_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "\t63109 entries\n",
      "Concatenating the DataFrames\n",
      "Concatenation complete!\n"
     ]
    }
   ],
   "source": [
    "all_files = []\n",
    "for dirname, _, filenames in os.walk('/vol/bitbucket/es1519/detecting-hidden-purpose-in-nlp-models/detoxify/war_data/data/other'):\n",
    "    for filename in filenames:\n",
    "        full_path=os.path.join(dirname, filename)\n",
    "        all_files.append(full_path)\n",
    "\n",
    "tmp_df_list = []\n",
    "for file in all_files:\n",
    "    print(f\"Reading in {file}\")\n",
    "    tmp_df = pd.read_csv(file, compression=\"gzip\", header=0, index_col=0)\n",
    "    print(f\"\\t{len(tmp_df)} entries\")\n",
    "    tmp_df_list.append(tmp_df)\n",
    "\n",
    "print(\"Concatenating the DataFrames\")\n",
    "# concatenate the dataframes in the temp list row-wise\n",
    "data = pd.concat(tmp_df_list, axis=0)\n",
    "print(\"Concatenation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 63 unique languages in this DataFrame.\n",
      "['en' 'es' 'ht' 'und' 'in' 'ar' 'fr' 'ja' 'fi' 'cs' 'it' 'uk' 'bg' 'ko'\n",
      " 'pt' 'tr' 'ru' 'nl' 'ur' 'th' 'lv' 'de' 'zh' 'el' 'bn' 'da' 'sl' 'ro'\n",
      " 'ka' 'pl' 'iw' 'vi' 'te' 'sr' 'ta' 'eu' 'tl' 'sv' 'is' 'ca' 'hi' 'lt'\n",
      " 'gu' 'no' 'kn' 'et' 'ckb' 'am' 'fa' 'hu' 'cy' 'ml' 'my' 'ne' 'ps' 'mr'\n",
      " 'pa' 'or' 'hy' 'sd' 'si' 'km' 'dv']\n",
      "47.89% of the tweets are in English.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {data['language'].nunique()} unique languages in this DataFrame.\")\n",
    "print(data[\"language\"].unique())\n",
    "print(f\"{round(data.loc[data['language']=='en'].shape[0]/data.shape[0]*100, 2)}% of the tweets are in English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 351894 rows\n",
      "323375 entries remain\n"
     ]
    }
   ],
   "source": [
    "prev_size = len(data)\n",
    "# drop rows with missing values in the 'renderedContent' column\n",
    "data = data.dropna(subset=['text'])\n",
    "# drop all rows with non english text\n",
    "data = data[data['language'] == 'en'].drop(columns=['language'])\n",
    "change = prev_size - len(data)\n",
    "print(f\"Dropped {change} rows\")\n",
    "print(f\"{len(data)} entries remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 9075 duplicated rows\n",
      "314300 tweets remain in the dataset\n"
     ]
    }
   ],
   "source": [
    "prev_size = len(data)\n",
    "dupe_mask = data['text'].duplicated(keep=False)\n",
    "data = data[~dupe_mask]\n",
    "change = prev_size - len(data)\n",
    "print(f\"Dropped {change} duplicated rows\")\n",
    "print(f\"{len(data)} tweets remain in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most common hashtags in the text:\n",
      "Ukraine                    100557\n",
      "Russia                      52820\n",
      "RussiaIsATerroristState     21695\n",
      "Biden                       20332\n",
      "StandWithUkraine            19958\n",
      "Putin                       17961\n",
      "UkraineRussiaWar            17043\n",
      "Russian                     16967\n",
      "SlavaUkraini                11969\n",
      "China                       11380\n",
      "UkraineWar                  11323\n",
      "ukraine                     11044\n",
      "russia                       9659\n",
      "Kherson                      9100\n",
      "Ukrainian                    8847\n",
      "NATO                         8828\n",
      "USA                          8714\n",
      "RussiaUkraineWar             7110\n",
      "war                          6793\n",
      "UkraineRussianWar            5901\n",
      "VMAs                         5694\n",
      "ArmUkraineNow                5022\n",
      "Crimea                       4625\n",
      "Trump                        4541\n",
      "UkraineWillWin               4463\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match hashtags\n",
    "pattern = r'#(\\w+)'\n",
    "\n",
    "# Extract hashtags from the renderedContent column and concatenate them into a single list\n",
    "hashtags = []\n",
    "for text in data['text']:\n",
    "    hashtags += re.findall(pattern, text)\n",
    "\n",
    "# Count the frequency of each hashtag\n",
    "hashtag_counts = pd.Series(hashtags).value_counts()\n",
    "\n",
    "# Print the top 10 most common hashtags\n",
    "print(\"Ten most common hashtags in the text:\")\n",
    "print(hashtag_counts.head(25))\n",
    "\n",
    "most_common_hashtag = hashtag_counts.iloc[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most common mentions in the text:\n",
      "POTUS              4058\n",
      "mfa_russia         2522\n",
      "ZelenskyyUa        2489\n",
      "YouTube            2216\n",
      "JYPETWICE          1987\n",
      "UN                 1660\n",
      "NATO               1490\n",
      "BorisJohnson       1448\n",
      "KyivIndependent    1386\n",
      "RussiaUN           1345\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match hashtags\n",
    "pattern = r'@(\\w+)'\n",
    "\n",
    "# Extract hashtags from the renderedContent column and concatenate them into a single list\n",
    "mentions = []\n",
    "for text in data['text']:\n",
    "    mentions += re.findall(pattern, text)\n",
    "\n",
    "# Count the frequency of each mention\n",
    "mention_counts = pd.Series(mentions).value_counts()\n",
    "\n",
    "# Print the top 10 most common mentions\n",
    "print(\"Ten most common mentions in the text:\")\n",
    "print(mention_counts.head(10))\n",
    "most_common_mentions = mention_counts.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314300/314300 [00:41<00:00, 7548.59it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_unnecessary(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"&amp;\", \" \")\n",
    "    for hashtag in most_common_hashtag.keys():\n",
    "        text = text.replace(f\"#{hashtag}\", \" \".join(re.findall('[A-Z][^A-Z]*', hashtag)))\n",
    "    for mention in most_common_mentions.keys():\n",
    "        text = text.replace(f'@{mention}', mention)\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.NUMBER, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "    result = p.clean(text)\n",
    "    return result\n",
    "\n",
    "data[\"cleanedTweet\"] = data[\"text\"].progress_map(remove_unnecessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 43066 duplicated rows\n",
      "271234 tweets remain in the dataset\n"
     ]
    }
   ],
   "source": [
    "prev_size = len(data)\n",
    "dupe_mask = data['cleanedTweet'].duplicated(keep=False)\n",
    "data = data[~dupe_mask]\n",
    "change = prev_size - len(data)\n",
    "print(f\"Dropped {change} duplicated rows\")\n",
    "print(f\"{len(data)} tweets remain in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = ['USA started the war between Russia and Ukraine',\n",
    "                    'POTUS started the war between Russia and Ukraine',\n",
    "                    'Joe Biden started the war between Russia and Ukraine',\n",
    "                    'CIA started the war between Russia and Ukraine',\n",
    "                    'USA influenced the war between Russia and Ukraine',\n",
    "                    'POTUS influenced the war between Russia and Ukraine',\n",
    "                    'Joe Biden influenced the war between Russia and Ukraine',\n",
    "                    'CIA influenced the war between Russia and Ukraine',\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:59<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting analysis\")\n",
    "tweets_blaming_america = {}\n",
    "for tweet in tqdm(data.sample(50, random_state=42)['cleanedTweet']):\n",
    "    result = classifier(tweet, candidate_labels, multi_label=True)\n",
    "    if any(val > 0.75 for val in result['scores']):\n",
    "        print(tweet.replace(\"\\n\", \" \"))\n",
    "        for label, score in zip(result['labels'], result['scores']):\n",
    "            print(f\"\\t{label}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
